{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T20:59:27.830048Z",
     "start_time": "2025-01-21T20:58:48.212210Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score, accuracy_score, precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Automatic File Loading ---\n",
    "def load_mat_files(directory):\n",
    "    data_frames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".mat\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            mat_data = loadmat(filepath)\n",
    "\n",
    "            # Extract relevant fields (customize based on data structure)\n",
    "            wind_coeff = mat_data.get('Wind_pressure_coefficients')\n",
    "            if wind_coeff is not None:\n",
    "                mean_pressure = wind_coeff.mean(axis=1)\n",
    "                features = {\n",
    "                    'Roof_pitch': mat_data.get('Roof_pitch', [0])[0],\n",
    "                    'Sample_frequency': mat_data.get('Sample_frequency', [0])[0],\n",
    "                    'Building_depth': mat_data.get('Building_depth', [0])[0],\n",
    "                    'Building_breadth': mat_data.get('Building_breadth', [0])[0],\n",
    "                    'Building_height': mat_data.get('Building_height', [0])[0],\n",
    "                    'Wind_azimuth': mat_data.get('Wind_azimuth', [0])[0],\n",
    "                    'Mean_pressure_coefficient': mean_pressure\n",
    "                }\n",
    "                df = pd.DataFrame(features)\n",
    "                data_frames.append(df)\n",
    "\n",
    "    return pd.concat(data_frames, ignore_index=True) if data_frames else None\n",
    "\n",
    "# Load all .mat files in directory\n",
    "data_directory = \"../data/Low-rise with eaves/roof type a/height 1;4/\"  # Replace with your directory\n",
    "all_data = load_mat_files(data_directory)\n",
    "if all_data is None:\n",
    "    raise ValueError(\"No valid data found in the directory.\")\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "X = all_data.drop(columns=['Mean_pressure_coefficient'])\n",
    "y = all_data['Mean_pressure_coefficient']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- PIML Dataset and Model ---\n",
    "class PressureDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(targets.values, dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "train_dataset = PressureDataset(X_train, y_train)\n",
    "test_dataset = PressureDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class PIMLModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PIMLModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(torch.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = PIMLModel(input_size=X_train.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train Model\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for features, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            outputs = model(features)\n",
    "            val_loss += criterion(outputs.squeeze(), targets).item()\n",
    "\n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        outputs = model(features)\n",
    "        y_pred.append(outputs.squeeze().cpu().numpy())\n",
    "        y_true.append(targets.cpu().numpy())\n",
    "\n",
    "y_pred = np.concatenate(y_pred)\n",
    "y_true = np.concatenate(y_true)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"PIML - MAE: {mae:.4f}, MAPE: {mape:.4f}, MSE: {mse:.4f}, RÂ²: {r2:.4f}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 45\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# Load all .mat files in directory\u001B[39;00m\n\u001B[0;32m     44\u001B[0m data_directory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/Low-rise with eaves/roof type a/height 1;4/\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Replace with your directory\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m all_data \u001B[38;5;241m=\u001B[39m load_mat_files(data_directory)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m all_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo valid data found in the directory.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1], line 38\u001B[0m, in \u001B[0;36mload_mat_files\u001B[1;34m(directory)\u001B[0m\n\u001B[0;32m     28\u001B[0m             mean_pressure \u001B[38;5;241m=\u001B[39m wind_coeff\u001B[38;5;241m.\u001B[39mmean(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     29\u001B[0m             features \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     30\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRoof_pitch\u001B[39m\u001B[38;5;124m'\u001B[39m: mat_data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRoof_pitch\u001B[39m\u001B[38;5;124m'\u001B[39m, [\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m     31\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSample_frequency\u001B[39m\u001B[38;5;124m'\u001B[39m: mat_data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSample_frequency\u001B[39m\u001B[38;5;124m'\u001B[39m, [\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     36\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMean_pressure_coefficient\u001B[39m\u001B[38;5;124m'\u001B[39m: mean_pressure\n\u001B[0;32m     37\u001B[0m             }\n\u001B[1;32m---> 38\u001B[0m             df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(features)\n\u001B[0;32m     39\u001B[0m             data_frames\u001B[38;5;241m.\u001B[39mappend(df)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mconcat(data_frames, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mif\u001B[39;00m data_frames \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CAPSTONE_PROJECT\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    772\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[0;32m    773\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[0;32m    774\u001B[0m     )\n\u001B[0;32m    776\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    777\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[1;32m--> 778\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m dict_to_mgr(data, index, columns, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy, typ\u001B[38;5;241m=\u001B[39mmanager)\n\u001B[0;32m    779\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mrecords\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CAPSTONE_PROJECT\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[1;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[0;32m    499\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    500\u001B[0m         \u001B[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001B[39;00m\n\u001B[0;32m    501\u001B[0m         arrays \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[1;32m--> 503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001B[38;5;241m=\u001B[39mdtype, typ\u001B[38;5;241m=\u001B[39mtyp, consolidate\u001B[38;5;241m=\u001B[39mcopy)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CAPSTONE_PROJECT\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verify_integrity:\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;66;03m# figure out the index, if necessary\u001B[39;00m\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 114\u001B[0m         index \u001B[38;5;241m=\u001B[39m _extract_index(arrays)\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    116\u001B[0m         index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CAPSTONE_PROJECT\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001B[0m, in \u001B[0;36m_extract_index\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    675\u001B[0m lengths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(raw_lengths))\n\u001B[0;32m    676\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(lengths) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 677\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll arrays must be of the same length\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    679\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m have_dicts:\n\u001B[0;32m    680\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    681\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    682\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: All arrays must be of the same length"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
