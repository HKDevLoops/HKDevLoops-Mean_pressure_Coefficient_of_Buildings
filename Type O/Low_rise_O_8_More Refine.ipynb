{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-16T19:26:26.753727Z",
     "start_time": "2025-03-16T19:26:22.953021Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error, \\\n",
    "    accuracy_score\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Data Loading Function ---\n",
    "def load_and_preprocess_data(directory):\n",
    "    \"\"\"\n",
    "    Load multiple .mat files, extract relevant features, and preprocess.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_targets = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".mat\"):\n",
    "            data_path = os.path.join(directory, file)\n",
    "            mat_data = loadmat(data_path)\n",
    "\n",
    "            # Extract fields\n",
    "            wind_pressure_coefficients = mat_data['Wind_pressure_coefficients']\n",
    "            roof_pitch = mat_data['Roof_pitch'].flatten()\n",
    "            building_depth = mat_data['Building_depth'].flatten()\n",
    "            building_breadth = mat_data['Building_breadth'].flatten()\n",
    "            building_height = mat_data['Building_height'].flatten()\n",
    "            wind_azimuth = mat_data['Wind_azimuth'].flatten()\n",
    "\n",
    "            mean_pressure_coefficients = wind_pressure_coefficients.mean(axis=1)\n",
    "\n",
    "            # Ensure consistent feature lengths\n",
    "            num_samples = len(mean_pressure_coefficients)\n",
    "            features = pd.DataFrame({\n",
    "                \"Roof_pitch\": np.tile(roof_pitch, num_samples // len(roof_pitch)),\n",
    "                \"Building_depth\": np.tile(building_depth, num_samples // len(building_depth)),\n",
    "                \"Building_breadth\": np.tile(building_breadth, num_samples // len(building_breadth)),\n",
    "                \"Building_height\": np.tile(building_height, num_samples // len(building_height)),\n",
    "                \"Wind_azimuth\": np.tile(wind_azimuth, num_samples // len(wind_azimuth)),\n",
    "            })\n",
    "            features[\"Mean_pressure_coefficient\"] = mean_pressure_coefficients\n",
    "\n",
    "            all_features.append(features.drop(columns=[\"Mean_pressure_coefficient\"]))\n",
    "            all_targets.append(features[\"Mean_pressure_coefficient\"])\n",
    "\n",
    "    # Combine all files\n",
    "    combined_features = pd.concat(all_features, axis=0, ignore_index=True)\n",
    "    combined_targets = pd.concat(all_targets, axis=0, ignore_index=True)\n",
    "    return combined_features, combined_targets\n",
    "\n",
    "# Load data\n",
    "data_dir = \"../data/Low-rise with eaves/\"  # Change to your directory path\n",
    "features, targets = load_and_preprocess_data(data_dir)\n",
    "\n",
    "# Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(features)\n",
    "feature_names = poly.get_feature_names_out(features.columns)\n",
    "poly_df = pd.DataFrame(poly_features, columns=feature_names)\n",
    "poly_df[\"Mean_pressure_coefficient\"] = targets\n",
    "\n",
    "# --- Correlation Analysis ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = poly_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "X = poly_df.drop(columns=[\"Mean_pressure_coefficient\"])\n",
    "y = poly_df[\"Mean_pressure_coefficient\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- XGBoost Model ---\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    max_depth=8,\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=50,\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# XGBoost Evaluation\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mape_xgb = mean_absolute_percentage_error(y_test, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost - MAE: {mae_xgb:.4f}, MAPE: {mape_xgb:.4f}, MSE: {mse_xgb:.4f}, R²: {r2_xgb:.4f}\")\n",
    "\n",
    "# --- PIML Neural Network ---\n",
    "class PressureDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "train_dataset = PressureDataset(X_train_scaled, y_train.values)\n",
    "test_dataset = PressureDataset(X_test_scaled, y_test.values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(torch.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = RegressionModel(input_size=X_train_scaled.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for features, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            outputs = model(features)\n",
    "            val_loss += criterion(outputs.squeeze(), targets).item()\n",
    "\n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot Training vs Validation Loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"PIML Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate PIML model\n",
    "y_pred_p = []\n",
    "y_true_p = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        outputs = model(features)\n",
    "        y_pred_p.append(outputs.squeeze().cpu().numpy())\n",
    "        y_true_p.append(targets.cpu().numpy())\n",
    "\n",
    "y_pred_p = np.concatenate(y_pred_p)\n",
    "y_true_p = np.concatenate(y_true_p)\n",
    "\n",
    "mae_piml = mean_absolute_error(y_true_p, y_pred_p)\n",
    "mape_piml = mean_absolute_percentage_error(y_true_p, y_pred_p)\n",
    "mse_piml = mean_squared_error(y_true_p, y_pred_p)\n",
    "r2_piml = r2_score(y_true_p, y_pred_p)\n",
    "accuracy_piml = accuracy_score((y_true_p > 0).astype(int), (y_pred_p > 0).astype(int))\n",
    "print(f\"PIML - MAE: {mae_piml:.4f}, MAPE: {mape_piml:.4f}, MSE: {mse_piml:.4f}, R²: {r2_piml:.4f} ,Accuracy: {accuracy_piml:.4f}\")\n",
    "\n",
    "# Comparison Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_xgb, alpha=0.5, color='red', edgecolor='k', label='XGBoost')\n",
    "plt.scatter(y_true_p, y_pred_p, alpha=0.5, color='blue', edgecolor='k', label='PIML')\n",
    "plt.xlabel(\"True Mean Pressure Coefficients\")\n",
    "plt.ylabel(\"Predicted Mean Pressure Coefficients\")\n",
    "plt.title(\"True vs Predicted Mean Pressure Coefficients\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 64\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# Load data\u001B[39;00m\n\u001B[0;32m     63\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/Low-rise with eaves/\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Change to your directory path\u001B[39;00m\n\u001B[1;32m---> 64\u001B[0m features, targets \u001B[38;5;241m=\u001B[39m load_and_preprocess_data(data_dir)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# Polynomial Features\u001B[39;00m\n\u001B[0;32m     67\u001B[0m poly \u001B[38;5;241m=\u001B[39m PolynomialFeatures(degree\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, include_bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[1], line 58\u001B[0m, in \u001B[0;36mload_and_preprocess_data\u001B[1;34m(directory)\u001B[0m\n\u001B[0;32m     55\u001B[0m         all_targets\u001B[38;5;241m.\u001B[39mappend(features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean_pressure_coefficient\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# Combine all files\u001B[39;00m\n\u001B[1;32m---> 58\u001B[0m combined_features \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(all_features, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     59\u001B[0m combined_targets \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(all_targets, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m combined_features, combined_targets\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001B[0m, in \u001B[0;36mconcat\u001B[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m copy \u001B[38;5;129;01mand\u001B[39;00m using_copy_on_write():\n\u001B[0;32m    380\u001B[0m     copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 382\u001B[0m op \u001B[38;5;241m=\u001B[39m _Concatenator(\n\u001B[0;32m    383\u001B[0m     objs,\n\u001B[0;32m    384\u001B[0m     axis\u001B[38;5;241m=\u001B[39maxis,\n\u001B[0;32m    385\u001B[0m     ignore_index\u001B[38;5;241m=\u001B[39mignore_index,\n\u001B[0;32m    386\u001B[0m     join\u001B[38;5;241m=\u001B[39mjoin,\n\u001B[0;32m    387\u001B[0m     keys\u001B[38;5;241m=\u001B[39mkeys,\n\u001B[0;32m    388\u001B[0m     levels\u001B[38;5;241m=\u001B[39mlevels,\n\u001B[0;32m    389\u001B[0m     names\u001B[38;5;241m=\u001B[39mnames,\n\u001B[0;32m    390\u001B[0m     verify_integrity\u001B[38;5;241m=\u001B[39mverify_integrity,\n\u001B[0;32m    391\u001B[0m     copy\u001B[38;5;241m=\u001B[39mcopy,\n\u001B[0;32m    392\u001B[0m     sort\u001B[38;5;241m=\u001B[39msort,\n\u001B[0;32m    393\u001B[0m )\n\u001B[0;32m    395\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001B[0m, in \u001B[0;36m_Concatenator.__init__\u001B[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[0m\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverify_integrity \u001B[38;5;241m=\u001B[39m verify_integrity\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy \u001B[38;5;241m=\u001B[39m copy\n\u001B[1;32m--> 445\u001B[0m objs, keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clean_keys_and_objs(objs, keys)\n\u001B[0;32m    447\u001B[0m \u001B[38;5;66;03m# figure out what our result ndim is going to be\u001B[39;00m\n\u001B[0;32m    448\u001B[0m ndims \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_ndims(objs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001B[0m, in \u001B[0;36m_Concatenator._clean_keys_and_objs\u001B[1;34m(self, objs, keys)\u001B[0m\n\u001B[0;32m    504\u001B[0m     objs_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(objs)\n\u001B[0;32m    506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(objs_list) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 507\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo objects to concatenate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m keys \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    510\u001B[0m     objs_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(com\u001B[38;5;241m.\u001B[39mnot_none(\u001B[38;5;241m*\u001B[39mobjs_list))\n",
      "\u001B[1;31mValueError\u001B[0m: No objects to concatenate"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c00c77b3a3949a70"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAPSTONE_PROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
